{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">Orestis Charalambous</h4> \n",
    "<h1><center><font size=\"50\">Task 2</font></center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [1 Q-learning introduction](#1)\n",
    "    * [1.1 Importing libraries](#1.1)\n",
    "    * [1.2 Parameters for Q learning](#1.2)\n",
    "    * [1.3 Construction of the Q table ](#1.3)\n",
    "    * [1.4 Training ](#1.4)\n",
    "    * [1.5 Evaluation and Results ](#1.5)\n",
    "* [2. Q-Learning with different Q_table and parameters](#2)\n",
    "    * [2.1 Modiying Environment](#2.1)\n",
    "    * [2.2 Hyperparameters for Q-Learning](#2.2)\n",
    "    * [2.3 Training](#2.3)\n",
    "    * [2.4 Evaluation](#2.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q Learning <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "attachments": {
    "406769f6-0406-4051-98b9-3d3815cb9b8b.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Q learning as our Reinforcement Learning algorithm for this task, the agent needs to know its state.\n",
    "\n",
    "In our environment we will use the relative coordinates between the player and the princess as the state.\n",
    "\n",
    "In this process we will generate two different q tables with the same maze and obstacles and look at differences.\n",
    "\n",
    "\n",
    "### A bit about Q learning\n",
    "\n",
    "\n",
    "Q learning is a model-free reinforcement learning algorithm. It will learn a policy which will tell it what to do given a certain situation. Over the course of training, the Q learning will update its policy to find the optimal (or the closest it can get) action given a state.\n",
    "\n",
    "![image.png](attachment:406769f6-0406-4051-98b9-3d3815cb9b8b.png)\n",
    "\n",
    "On every run the Q table is updated with a new Q value. This is defined in the above equation. This takes the existing value and multiplies it by the learned value. The learned value is a compibation of the reward from the latest move and the maximum Q value from the new state. [Source](https://en.wikipedia.org/wiki/Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Importing libraries<a class=\"anchor\" id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env                     # Importing the environment from py file.\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pickle                  # Importing pickle to store the q table.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Customising style for Graphs\n",
    "plt.style.use(['ggplot'])\n",
    "plt.rcParams['axes.edgecolor']='#333F4B'\n",
    "plt.rcParams['axes.linewidth']=0.8\n",
    "plt.rcParams['xtick.color']='#333F4B'\n",
    "plt.rcParams['ytick.color']='#333F4B'\n",
    "plt.rcParams['grid.color']='#dcdcdc'\n",
    "plt.rcParams['axes.facecolor']='white'\n",
    "mpl.rcParams['font.size'] = 15\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.2 Parameters for Q learning <a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 7                   # Size of maze as a square (NxN).\n",
    "NUM_EPISODES =10000          # Number of episodes.\n",
    "SHOW_EVERY = 500            # Show epsilon and mean rewards every n episodes.\n",
    "epsilon = 0.9               # Exploration rate.\n",
    "DECAY_RATE = 0.9998          # Exploration decay rate.\n",
    "load_q_table = None\n",
    "episode_rewards = []\n",
    "LEARNING_RATE = 0.1         # Learning rate.\n",
    "gamma = 0.95                # Discounting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating random castle maze\n",
    "\n",
    "We will use this so our maze stays the same for all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXX', 'X XP  X', 'X C   X', 'X X X X', 'X   XXX', 'XA    X', 'XXXXXXX']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "castle = env.maze(SIZE)\n",
    "castle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Construction of the Q table <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "The observation space is going to be a coordinate which is the Delta, relative coordinates to the princess. This is given by substracting the coordinates of the agent with the coordinates of the princess.\n",
    "\n",
    "We are iterating through the observation space and we are adding every combination to our table with a tuple and initialising it with the numpy random uniform distribution for the discrete 4 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_q_table is None:\n",
    "    q_table = {}\n",
    "    for x1 in range(-SIZE + 1, SIZE):\n",
    "        for y1 in range(-SIZE + 1, SIZE):\n",
    "            for x2 in range(-SIZE + 1, SIZE):\n",
    "                for y2 in range(-SIZE + 1, SIZE):\n",
    "                    q_table[((x1, y1),(x2, y2))] = [np.random.uniform(-5, 0) for i in range(4)]\n",
    "else:\n",
    "    with open(load_q_table, \"rb\") as f:\n",
    "        q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Training <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "Here we will implement the Q-Learning algorithm with the epsilon greedy policy.\n",
    "\n",
    "We start by choosing a random number between (0,1). If that number is less than epsilon, the exploration rate, we explore with a random action. On the other hand, if the random number is greater than epsilon, we choose an action based on the current state of the agent based on the class with the largest predicted probability of our Q-table.\n",
    "\n",
    "The Agent makes that action and a new Q value is updated on the Q-table based on the new observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>WARNING: THIS CAN TAKE UP TO HALF AN HOUR TO RUN FOR 10000 EPISODES!!!</b>\n",
    "\n",
    "<b>Run time is caused because turtle graphics updates in lower frames than any other library or numpys arrays. This was not known at the beginning of the project and I apologise for the inconvinience.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Terminator",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminator\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7dcea5cbf49d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mnew_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1301\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tracing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturtles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1303\u001b[1;33m             \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m             \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drawturtle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tracing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36m_update_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_incrementudc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_updatecounter\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36m_incrementudc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTurtleScreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RUNNING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m             \u001b[0mTurtleScreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RUNNING\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTerminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tracing\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_updatecounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTerminator\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(NUM_EPISODES):\n",
    "    episode_reward = 0\n",
    "    env.wn.reset()\n",
    "    castle = env.maze(SIZE)\n",
    "    env.reset(castle)\n",
    "    for step in range(100):                         # Maximum steps.\n",
    "        env.wn.update()                             # Update screen.\n",
    "        state = env.observations()\n",
    "        if random.uniform(0, 1) > epsilon:            # If random number is greater than epsilon ---> explore.\n",
    "            action = np.argmax(q_table[state])      # Choose action based on the biggest Q value at this state.\n",
    "        else:\n",
    "            action = random.randint(0, 3)           # Choose random action (up, down, left, right), hence explore.\n",
    "\n",
    "        obs, reward, done = env.step(action)        # Obs is the relative position(Delta) between the agent and the enemy,princess.\n",
    "\n",
    "        if reward == env.princess.gold:\n",
    "            new_q = env.princess.gold\n",
    "        elif reward == env.treasure.gold:\n",
    "            new_q = env.treasure.gold\n",
    "        else:    \n",
    "            new_q = (1 - LEARNING_RATE) *q_table[state][action] + LEARNING_RATE * (reward + gamma * np.max(q_table[obs]))\n",
    "        q_table[obs][action] = new_q\n",
    "        env.wn.update()\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(episode_reward)           # Appending each episode reward to a list so we get the mean reward.\n",
    "    epsilon *= DECAY_RATE                            # Decaying the exploration rate every episode.\n",
    "    \n",
    "    if episode == 0:\n",
    "        pass\n",
    "    elif episode % SHOW_EVERY == 0:\n",
    "        print(\"Episode {}\\tMean reward of last {} episodes: {:.2f}\\tepsilon: {}\".format(episode,SHOW_EVERY, np.mean(episode_rewards[-SHOW_EVERY:]), epsilon))\n",
    "\n",
    "env.wn.bye()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Evaluation <a class=\"anchor\" id=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,)) / SHOW_EVERY, mode=\"valid\")\n",
    "\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.ylabel(\"Reward for {}\".format(SHOW_EVERY))\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning with different Q_table and parameters <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will copy our environment to make some modification to the observation value the Q table and the position of the agent will be random in each episode while the position of the princess, the enemy and the coin will not change.\n",
    "\n",
    "**Additions in functions:**\n",
    "\n",
    "- reset: Resets agent in random position\n",
    "- observation: Gets agent current position in index form \n",
    "- setup_maze: Creates the Q table with appended walls.\n",
    "- Enemy: Enemy moves randomly in maze\n",
    "\n",
    "\n",
    "[2.1 Modifying Environment](#2.1)\n",
    "\n",
    "[2.2 Hyperparameters for Q-Learning ](#2.2)\n",
    "\n",
    "[2.3 Training](#2.3)\n",
    "\n",
    "[2.4 Evaluation](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Modifying Environment <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get started with interactive Python\n",
    "import numpy as np\n",
    "import turtle\n",
    "import time\n",
    "import random\n",
    "import pickle                  # Importing pickle to store the q table.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# create pen\n",
    "class Pen(turtle.Turtle):\n",
    "    def __init__(self):\n",
    "        turtle.Turtle.__init__(self)\n",
    "        self.shape(\"square\")\n",
    "        self.color(\"white\")\n",
    "        self.penup()\n",
    "        self.speed(0)\n",
    "\n",
    "\n",
    "class Player(turtle.Turtle):\n",
    "    def __init__(self):\n",
    "        turtle.Turtle.__init__(self)\n",
    "        self.shape(\"player_down.gif\")   # Image of Agent.\n",
    "        self.color(\"blue\")              # Initial colour before graphics.\n",
    "        self.penup()\n",
    "        self.speed('fastest')           # Drawing speed.\n",
    "\n",
    "    def go_up(self):\n",
    "        move_to_x = player.xcor()\n",
    "        move_to_y = player.ycor() + 24\n",
    "        self.shape(\"player_up.gif\")\n",
    "        if (move_to_x, move_to_y) not in walls:\n",
    "            self.goto(move_to_x, move_to_y)\n",
    "\n",
    "    def go_down(self):\n",
    "        move_to_x = player.xcor()\n",
    "        move_to_y = player.ycor() - 24\n",
    "        self.shape(\"player_down.gif\")\n",
    "        if (move_to_x, move_to_y) not in walls:\n",
    "            self.goto(move_to_x, move_to_y)\n",
    "\n",
    "    def go_right(self):\n",
    "        move_to_x = player.xcor() + 24\n",
    "        move_to_y = player.ycor()\n",
    "        self.shape(\"player_right.gif\")\n",
    "        if (move_to_x, move_to_y) not in walls:\n",
    "            self.goto(move_to_x, move_to_y)\n",
    "\n",
    "    def go_left(self):\n",
    "        move_to_x = player.xcor() - 24\n",
    "        move_to_y = player.ycor()\n",
    "        self.shape(\"player_left.gif\")\n",
    "\n",
    "        if (move_to_x, move_to_y) not in walls:\n",
    "            self.goto(move_to_x, move_to_y)\n",
    "    \n",
    "    def is_collision(self, other):\n",
    "        if player.distance(other) < 5:  # Calculate if objects collide.\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class Princess(turtle.Turtle):          # Define Princess.\n",
    "    def __init__(self):\n",
    "        turtle.Turtle.__init__(self)\n",
    "        self.shape(\"princess.gif\")      # Image of princess.\n",
    "        self.color(\"yellow\")            # Initial colour before graphics.\n",
    "        self.penup()\n",
    "        self.gold = 300                 # Used as a reward when agent collides with princess.\n",
    "        self.speed(\"fastest\")           # Drawing speed.\n",
    "\n",
    "\n",
    "class Enemy(turtle.Turtle):             # Define Enemy.\n",
    "    def __init__(self):\n",
    "        turtle.Turtle.__init__(self)\n",
    "        self.shape(\"enemy_down.gif\")    # Image of enemy.\n",
    "        self.color(\"red\")               # Initial colour before graphics.\n",
    "        self.penup()\n",
    "        self.speed(\"fastest\")           # Drawing speed.\n",
    "        self.gold = 500                 # Used as a penalty when agent collides with enemy.\n",
    "        self.direction = random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "\n",
    "    def move(self):                 # Coordinates are in pixels.\n",
    "        if self.direction == \"up\":\n",
    "            dx = 0\n",
    "            dy = 24\n",
    "            self.shape(\"enemy_up.gif\")\n",
    "        elif self.direction == \"down\":\n",
    "            dx = 0\n",
    "            dy = -24\n",
    "            self.shape(\"enemy_down.gif\")\n",
    "        elif self.direction == \"left\":\n",
    "            dx = -24\n",
    "            dy = 0\n",
    "            self.shape(\"enemy_left.gif\")\n",
    "        elif self.direction == \"right\":\n",
    "            dx = 24\n",
    "            dy = 0\n",
    "            self.shape(\"enemy_right.gif\")\n",
    "        else:\n",
    "            dx = 0\n",
    "            dy = 0\n",
    "\n",
    "        # Calculate the spot to move to.\n",
    "        move_to_x = self.xcor() + dx\n",
    "        move_to_y = self.ycor() + dy\n",
    "\n",
    "        if (move_to_x, move_to_y) not in walls:     # Check if the spot is a wall.\n",
    "            self.goto(move_to_x, move_to_y)\n",
    "        else:\n",
    "            self.direction = random.choice([\"up\", \"down\", \"left\", \"right\"])  # Choose a different direction.\n",
    "\n",
    "        turtle.ontimer(self.move, t=random.randint(100, 300))                # Set timer for the next move.\n",
    "\n",
    "\n",
    "def maze(size):                 # Used to create maze grid.\n",
    "\n",
    "    map_characters = {0: ' ',   # Empty Space\n",
    "                      1: 'X',   # Walls\n",
    "                      2: 'A',   # Agent\n",
    "                      3: 'E',   # Enemy\n",
    "                      4: 'C',   # Coins\n",
    "                      5: 'P'}   # Princess\n",
    "\n",
    "    grid = []\n",
    "    for r in range(size):                       # Creating empty space Grid.\n",
    "        grid.append([])\n",
    "        for c in range(size):\n",
    "            grid[r].append(map_characters[0])\n",
    "\n",
    "    for r in range(len(grid)):                  # Creates the first and last column border.\n",
    "        grid[r][0] = map_characters[1]\n",
    "        grid[r][len(grid[r]) - 1] = map_characters[1]\n",
    "\n",
    "    for c in range(1, len(grid[0]) - 1):        # Creates the first and last row border.\n",
    "        grid[0][c] = map_characters[1]\n",
    "        grid[len(grid) - 1][c] = map_characters[1]\n",
    "\n",
    "    def grid_recursion(grid_, top_border, bottom_border, first_col_border, last_col_border):\n",
    "\n",
    "        # Figure out where to divide horizontally\n",
    "        beginning_point = bottom_border + 2\n",
    "        finishing_point = top_border - 1\n",
    "        chosen_col = random.randrange(beginning_point, finishing_point, 2)\n",
    "\n",
    "        # Do the division\n",
    "        for column in range(first_col_border + 1, last_col_border):\n",
    "            grid_[chosen_col][column] = map_characters[1]\n",
    "\n",
    "        # Figure out where to divide vertically\n",
    "        beginning_point = first_col_border + 2\n",
    "        finishing_point = last_col_border - 1\n",
    "        chosen_row = random.randrange(beginning_point, finishing_point, 2)\n",
    "\n",
    "        # Do the division\n",
    "        for row in range(bottom_border + 1, top_border):\n",
    "            grid_[row][chosen_row] = map_characters[1]\n",
    "\n",
    "        # Now we'll make a gap on 3 of the 4 walls.\n",
    "        # Figure out which wall does NOT get a gap.\n",
    "        new_border = random.randrange(4)\n",
    "        if new_border != 0:\n",
    "            space = random.randrange(first_col_border + 1, chosen_row, 2)\n",
    "            grid_[chosen_col][space] = map_characters[0]\n",
    "            grid_[chosen_col][space + 1] = map_characters[0]\n",
    "            grid_[chosen_col][space + 2] = map_characters[0]\n",
    "            grid_[chosen_col][space + 2] = map_characters[0]\n",
    "\n",
    "        if new_border != 1:\n",
    "            space = random.randrange(chosen_row + 1, last_col_border, 2)\n",
    "            grid_[chosen_col][space] = map_characters[0]\n",
    "            grid_[chosen_col][space - 1] = map_characters[0]\n",
    "            grid_[chosen_col][space - 2] = map_characters[0]\n",
    "            grid_[chosen_col][space - 2] = map_characters[0]\n",
    "\n",
    "        if new_border != 2:\n",
    "            space = random.randrange(bottom_border + 1, chosen_col, 2)\n",
    "            grid_[space][chosen_row] = map_characters[0]\n",
    "            grid_[space][chosen_row + 1] = map_characters[0]\n",
    "            grid_[chosen_col][space + 2] = map_characters[0]\n",
    "\n",
    "        if new_border != 3:\n",
    "            space = random.randrange(chosen_col + 1, top_border, 2)\n",
    "            grid_[space][chosen_row] = map_characters[0]\n",
    "            grid_[space][chosen_row - 1] = map_characters[0]\n",
    "            grid_[chosen_col][space - 2] = map_characters[0]\n",
    "\n",
    "        # If there's enough space, to a recursive call.\n",
    "        if top_border > chosen_col + 3 and chosen_row > first_col_border + 3:\n",
    "            grid_recursion(grid_, top_border, chosen_col, first_col_border, chosen_row)\n",
    "\n",
    "        if top_border > chosen_col + 3 and chosen_row + 3 < last_col_border:\n",
    "            grid_recursion(grid_, top_border, chosen_col, chosen_row, last_col_border)\n",
    "\n",
    "        if bottom_border + 3 < chosen_col and chosen_row + 3 < last_col_border:\n",
    "            grid_recursion(grid_, chosen_col, bottom_border, chosen_row, last_col_border)\n",
    "\n",
    "        if bottom_border + 3 < chosen_col and chosen_row > first_col_border + 3:\n",
    "            grid_recursion(grid_, chosen_col, bottom_border, first_col_border, chosen_row)\n",
    "\n",
    "        return grid_\n",
    "\n",
    "    recursion = grid_recursion(grid, size - 1, 0, 0, size - 1)      # Doing recursion again.\n",
    "\n",
    "    complete_castle = []\n",
    "    \n",
    "    for i in range(1):      # Assigning random positions for Enemy, Coin, Agent and Princess.\n",
    "        recursion[random.randint(2, size - 4)][random.randint(2, size - 2)] = map_characters[3]\n",
    "        recursion[size - 2][random.randint(1, size - 5)] = map_characters[2]  # Agent starts at the bottom of the maze.\n",
    "        recursion[random.randint(1, size - 13)][random.randint(2, size - 13)] = map_characters[5]        # Princess is at the top of the maze.\n",
    "\n",
    "    for i, j in enumerate(recursion):       # Organising the list to (size) strings for easier turtle access.\n",
    "        joined = ''.join(recursion[i])\n",
    "        complete_castle.append(joined)\n",
    "\n",
    "    return complete_castle                  # Returning complete maze.\n",
    "\n",
    "\n",
    "\n",
    "def observations():\n",
    "    global size\n",
    "    coord = player.pos()\n",
    "\n",
    "    xcor = (coord[0] + (((size - 1) * 24) / 2)) / 24\n",
    "    ycor = ((((size - 1) * 24) / 2) - coord[1]) / 24\n",
    "\n",
    "    return int(xcor), int(ycor)\n",
    "\n",
    "\n",
    "def step(action):\n",
    "    global q_tbl\n",
    "    pos = observations()\n",
    "    \n",
    "    if action == 0:\n",
    "        player.go_up()\n",
    "        q_tbl[pos[1]][pos[0]]['action'] == 'up'\n",
    "    elif action == 1:\n",
    "        player.go_down()\n",
    "        q_tbl[pos[1]][pos[0]]['action'] == 'down'\n",
    "    elif action == 2:\n",
    "        player.go_left()\n",
    "        q_tbl[pos[1]][pos[0]]['action'] == 'left'\n",
    "    elif action == 3:\n",
    "        player.go_right()\n",
    "        q_tbl[pos[1]][pos[0]]['action'] == 'right'\n",
    "\n",
    "    if player.is_collision(princess):\n",
    "        reward = princess.gold\n",
    "        done = True\n",
    "    elif player.is_collision(enemy):\n",
    "        reward = -enemy.gold\n",
    "        done = True\n",
    "    elif pos == observations():      # Hit wall\n",
    "        reward = -1\n",
    "        done = False\n",
    "    else:\n",
    "        reward = -0.04               # moving penalty\n",
    "        done = False\n",
    "\n",
    "    obs = observations()\n",
    "\n",
    "    return obs, reward, done\n",
    "\n",
    "\n",
    "def reset():  # Reset function.\n",
    "    pen.penup()                           \n",
    "    x = random.randint(0, 24)      # Assigning random X and Y coordinates for the position of the agent upon reseting of the environment.\n",
    "    y = random.randint(0, 24)\n",
    "    screen_x = -288 + (x * 24)\n",
    "    screen_y = 288 - (y * 24)\n",
    "    if (screen_x, screen_y) not in walls:\n",
    "        player.goto(screen_x, screen_y)\n",
    "\n",
    "\n",
    "# Create Level Setup Function\n",
    "def setup_maze(level):\n",
    "    global size\n",
    "    empty_q_table= []\n",
    "    for y in range(len(level)):                  # Select each line in the level.\n",
    "        empty_q_table.append([])                 # Create Q table\n",
    "        for x in range(len(level[y])):           # Select each character in the level.\n",
    "            character = level[y][x]              # Assigning the above character to a variable character.\n",
    "\n",
    "            screen_x = -(((size-1)*24)/2) + (x * 24)\n",
    "            screen_y = ((size-1)*24)/2 - (y * 24)\n",
    "            # Check if it is an X (representing a wall)\n",
    "            if character == \"X\":\n",
    "                pen.goto(screen_x, screen_y)                            # If character is \"X\", assign it as a wall.\n",
    "                pen.shape(\"wall.gif\")\n",
    "                pen.stamp()                                             # Leave an impression on the screen.\n",
    "                walls.append((screen_x, screen_y))                      # Add coordinate to wall list.\n",
    "                empty_q_table[y].append({ \"q\" : 0 , \"action\" : \"X\"})\n",
    "            else:\n",
    "                empty_q_table[y].append({ \"q\" : [0,0,0,0], \"action\" : \"\"})\n",
    "\n",
    "            if character == \"A\":\n",
    "                player.goto(screen_x, screen_y)                     # If character is \"A\", assign it as an Agent.\n",
    "\n",
    "            if character == \"P\":\n",
    "                princess.goto(screen_x, screen_y)                   # If character is \"P\", assign it as a Princess.\n",
    "                princess.stamp()\n",
    "            \n",
    "            if character == \"E\":\n",
    "                enemy.goto(screen_x, screen_y)                      # If character is \"E\", assign it as an Enemy.\n",
    "\n",
    "    return empty_q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Hyperparameters for Q-Learning <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "We decreased the epsilon decay rate because our model trains the agent better by exploring the  environment rather than repititions of episodes, therefore it will take more steps per episodes than in the first model so we want the agent to choose Q values in early episodes. The trainining process will terminate if the agent has a mean reward of 280(needs around 20 steps to reach the reward) for the previous 20 episodes. The maximum reward is 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "size = 25\n",
    "NUM_EPISODES = 300          # Number of episodes.\n",
    "SHOW_EVERY = 10            # Show epsilon and mean rewards every n episodes.\n",
    "epsilon = 0.9               # Exploration rate.\n",
    "DECAY_RATE = 0.966          # Exploration decay rate.\n",
    "episode_rewards = []\n",
    "LEARNING_RATE = 0.1         # Learning rate.\n",
    "gamma = 0.9                 # Discounting rate\n",
    "n_episodes = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wn = turtle.Screen()            # Define turtle screen.\n",
    "wn.bgcolor(\"black\")             # Set Background color.\n",
    "wn.title(\"Mission Impossible\")  # Title of Maze.\n",
    "wn.setup(700, 700)              # Setup dimensions of working window.\n",
    "wn.tracer(0)                    # Instant display.\n",
    "\n",
    "images = [\"wall.gif\", \"enemy_down.gif\", \"enemy_up.gif\", \"enemy_left.gif\", \"enemy_right.gif\",\n",
    "          \"gold.gif\", \"player_down.gif\", \"princess.gif\", \"player_left.gif\", \"player_up.gif\", \"player_right.gif\"]\n",
    "\n",
    "for image in images:\n",
    "    turtle.register_shape(image)   # Register graphics in turtle.\n",
    "\n",
    "\n",
    "# Create class instances\n",
    "pen = Pen()\n",
    "princess = Princess()\n",
    "player = Player()\n",
    "enemy = Enemy()\n",
    "\n",
    "# Create wall list to append coordinates.\n",
    "walls = []\n",
    "\n",
    "castle = maze(size)\n",
    "\n",
    "q_tbl = setup_maze(castle)\n",
    "\n",
    "turtle.ontimer(enemy.move, t=250)\n",
    "\n",
    "wn.tracer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Training <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our agent so he can be able choose the closest path from any position on the maze towards the princess without touching the enemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------Training the agent----------------------------------------------\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8c955b9028e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# Choose random action (up, down, left, right), hence explore.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprincess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-39eee01d1949>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mq_tbl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'action'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'up'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgo_down\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mq_tbl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'action'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'down'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-39eee01d1949>\u001b[0m in \u001b[0;36mgo_down\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"player_down.gif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmove_to_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_to_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwalls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove_to_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_to_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgo_right\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36mgoto\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_goto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVec2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_goto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVec2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36m_goto\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m   3156\u001b[0m                       (self.currentLineItem,\n\u001b[0;32m   3157\u001b[0m                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentLine\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3158\u001b[1;33m                       \u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pointlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentLineItem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3159\u001b[0m                       self.items[:])\n\u001b[0;32m   3160\u001b[0m                       )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\turtle.py\u001b[0m in \u001b[0;36m_pointlist\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    753\u001b[0m         (9.9999999999999982, 0.0)]\n\u001b[0;32m    754\u001b[0m         >>> \"\"\"\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[0mpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mreturn\u001b[0m  \u001b[0mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args, **kw)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2759\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2760\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2761\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2763\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------Training the agent----------------------------------------------\")\n",
    "\n",
    "start_time = time.time()  \n",
    "end = False\n",
    "\n",
    "while not end:\n",
    "    episode_reward = 0\n",
    "    n_episodes +=1\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while not done:  # Maximum steps.\n",
    "        \n",
    "        wn.update()  # Update screen.\n",
    "        \n",
    "        state = observations()                                 # Get current state.\n",
    "        \n",
    "        if random.uniform(0, 1) > epsilon:                      # If random number is lower than epsilon ---> explore.\n",
    "            action = np.argmax(q_tbl[state[1]][state[0]]['q'])  # Choose action based on the biggest Q value at this state.\n",
    "        else:\n",
    "            action = random.randint(0, 3)                       # Choose random action (up, down, left, right), hence explore.\n",
    "\n",
    "        obs, reward, done = step(action)\n",
    "        \n",
    "        if reward == princess.gold:\n",
    "            max_q_value_move = q_tbl[obs[1]][obs[0]]['q'][np.argmax(q_tbl[obs[1]][obs[0]]['q'])]  # Princess gold.\n",
    "        elif reward == -1:\n",
    "            max_q_value_move = q_tbl[state[1]][state[0]]['q'][action]                             # Hits a wall.\n",
    "        elif reward == -0.04:\n",
    "            max_q_value_move = q_tbl[obs[1]][obs[0]]['q'][np.argmax(q_tbl[obs[1]][obs[0]]['q'])]  # Moving penalty.\n",
    "        elif reward == -enemy.gold:\n",
    "            max_q_value_move = q_tbl[obs[1]][obs[0]]['q'][np.argmax(q_tbl[obs[1]][obs[0]]['q'])]  # Enemy penalty.\n",
    " \n",
    "        new_q = q_tbl[state[1]][state[0]]['q'][action] + (alpha * (reward + (gamma * max_q_value_move) - q_tbl[state[1]][state[0]]['q'][action]))\n",
    "        q_tbl[state[1]][state[0]]['q'][action] = new_q         # Update Q table\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    reset()\n",
    "    episode_rewards.append(episode_reward)  # Appending each episode reward to a list so we get the mean reward.\n",
    "    epsilon *= DECAY_RATE                   # Decaying the exploration rate every episode.\n",
    "\n",
    "    if n_episodes % SHOW_EVERY == 0:\n",
    "        print(\"Episode {}\\tMean reward of last {} episodes: {:.2f}\\tepsilon: {}\".format(n_episodes, SHOW_EVERY, np.mean(\n",
    "            episode_rewards[-SHOW_EVERY:]), epsilon))\n",
    "    elif np.mean(episode_rewards[-20:]) > 280:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time       # How much time did the agent trained for.\n",
    "        end = True\n",
    "\n",
    "wn.bye()\n",
    "\n",
    "print(\"----------------------------------Training complete--------------------------------------\")\n",
    "print(\"------------------Time trained: {:.1f} seconds   Mean Reward: {:.2f}for {} episodes------------------\".format(elapsed_time, np.mean(episode_rewards), n_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Evaluation <a class=\"anchor\" id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,)) / SHOW_EVERY, mode=\"valid\")\n",
    "\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see that our model does not take too long to learn the maze. It just need some time to discover the princess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
